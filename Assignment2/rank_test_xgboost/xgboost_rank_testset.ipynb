{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_hdf(\"../data/traindf_clean.hdf\")\n",
    "testdf = pd.read_hdf(\"../data/test_clean.hdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(traindf.columns))\n",
    "print(list(testdf.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = traindf.sort_values(by=['srch_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split label and other variables\n",
    "x_train, y_train = traindf, traindf[\"importance\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    shuffle=False, \n",
    "                                                    stratify = None)\n",
    "\n",
    "# Drop columns that are to be predicted with importance (which is set to y_train)\n",
    "x_train = x_train.drop(columns=[\"position\", \"importance\", \"booking_bool\", \"click_bool\"])\n",
    "\n",
    "# x_test is the total testset with all columns. \n",
    "x_test = testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correct shape and columns;\n",
    "# x_train is probably much smaller because of downsampling: (50% of importance 5 and 1, 50% importance 0). \n",
    "# Number of columns in x_train and x_test must be equal!\n",
    "print(x_train.shape, y_train.shape, x_test.shape)\n",
    "print(list(x_train.columns))\n",
    "print(list(x_test.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: parameter tuning!\n",
    "params = {'objective': 'rank:pairwise', 'learning_rate': 0.1,\n",
    "          'gamma': 1.0, 'min_child_weight': 0.1,\n",
    "          'max_depth': 6,  'n_estimators': 500}\n",
    "\n",
    "# groups are equal to length of unique queries\n",
    "query_lengths = x_train.groupby('srch_id').size().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.sklearn.XGBRanker(**params)\n",
    "model.fit(x_train, y_train, query_lengths, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make fake column of positions to test score in ndcg scoring function,\n",
    "# This is not needed for the testset, because there is nothing to check in the testset\n",
    "# x_train[\"position_temp\"] = x_train.groupby(['srch_id']).cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train[\"position\"] = y_train\n",
    "predictions_sorted = []\n",
    "prop_ids_sorted = []\n",
    "\n",
    "# Sort predictions for each group SEPERATELY\n",
    "for srchid, group in x_test.groupby('srch_id'):\n",
    "    \n",
    "    # Predictions for one search_id\n",
    "    pred = model.predict(group)\n",
    "    \n",
    "    prop_id = [x for _,x in sorted(zip(pred, group.prop_id), reverse=True)]\n",
    "    prop_ids_sorted.append(prop_id)\n",
    "    \n",
    "# Flatten lists\n",
    "predictions_sorted = [item for sublist in predictions_sorted for item in sublist]\n",
    "prop_ids_sorted = [item for sublist in prop_ids_sorted for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[\"prop_id\"] = prop_ids_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set only: convert to csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"submission9mei.csv\"\n",
    "final_df = x_test[[\"srch_id\", \"prop_id\"]]\n",
    "final_df.to_csv(filename, columns=[\"srch_id\", \"prop_id\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test if it worked\n",
    "test = pd.read_csv(filename)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = pd.DataFrame(x_test.columns, model.feature_importances_)\n",
    "# importances.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
