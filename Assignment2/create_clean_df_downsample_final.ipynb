{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kies testing_set is False of True!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIER!!!!!!\n",
    "testing_set = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "50\n",
      "Index(['srch_id', 'date_time', 'site_id', 'visitor_location_country_id',\n",
      "       'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_country_id',\n",
      "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
      "       'prop_location_score1', 'prop_location_score2',\n",
      "       'prop_log_historical_price', 'position', 'price_usd', 'promotion_flag',\n",
      "       'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
      "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
      "       'srch_saturday_night_bool', 'srch_query_affinity_score',\n",
      "       'orig_destination_distance', 'random_bool', 'comp1_rate', 'comp1_inv',\n",
      "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
      "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
      "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
      "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
      "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
      "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
      "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
      "       'comp8_rate_percent_diff', 'click_bool', 'gross_bookings_usd',\n",
      "       'booking_bool'],\n",
      "      dtype='object')\n",
      "Index(['srch_id', 'date_time', 'site_id', 'visitor_location_country_id',\n",
      "       'visitor_hist_starrating', 'visitor_hist_adr_usd', 'prop_country_id',\n",
      "       'prop_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
      "       'prop_location_score1', 'prop_location_score2',\n",
      "       'prop_log_historical_price', 'price_usd', 'promotion_flag',\n",
      "       'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window',\n",
      "       'srch_adults_count', 'srch_children_count', 'srch_room_count',\n",
      "       'srch_saturday_night_bool', 'srch_query_affinity_score',\n",
      "       'orig_destination_distance', 'random_bool', 'comp1_rate', 'comp1_inv',\n",
      "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
      "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
      "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
      "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
      "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
      "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
      "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
      "       'comp8_rate_percent_diff'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# testdf = pd.read_hdf(\"./data/corrected_price_testset.hdf\")\n",
    "# traindf = pd.read_hdf(\"./data/corrected_price.hdf\")\n",
    "\n",
    "# testdf = pd.read_hdf(\"./data/corrected_price_testset_rollback_and_avg.hdf\")\n",
    "\n",
    "# traindf = pd.read_hdf(\"./data/corrected_price_rollback_and_avg.hdf\")\n",
    "\n",
    "testdf = pd.read_csv(\"data/test_set_VU_DM.csv\")\n",
    "traindf = pd.read_csv(\"data/training_set_VU_DM.csv\")\n",
    "\n",
    "print(len(traindf.columns))\n",
    "print(len(testdf.columns))\n",
    "\n",
    "print(traindf.columns)\n",
    "print(testdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage test:  46.3779\n",
      "Percentage train:  44.7182\n",
      "4959183\n",
      "4958347\n",
      "199549\n",
      "199795\n",
      "129438\n",
      "129113\n"
     ]
    }
   ],
   "source": [
    "def calculate_missing_percentage(df):\n",
    "    return ((df.isna().mean() * 100).sum() / len(df.columns)).round(4)\n",
    "\n",
    "print(\"Percentage test: \", calculate_missing_percentage(testdf))\n",
    "print(\"Percentage train: \", calculate_missing_percentage(traindf))\n",
    "\n",
    "\n",
    "print(len(testdf))\n",
    "print(len(traindf))\n",
    "\n",
    "print(len(testdf.srch_id.unique()))\n",
    "print(len(traindf.srch_id.unique()))\n",
    "\n",
    "print(len(testdf.prop_id.unique()))\n",
    "print(len(traindf.prop_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage train:  0.7907\n"
     ]
    }
   ],
   "source": [
    "traindf2 = competitors(traindf)\n",
    "traindf2 = traindf2.drop(columns=['comp1_rate', 'comp1_inv',\n",
    "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
    "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
    "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
    "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
    "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
    "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
    "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
    "       'comp8_rate_percent_diff'])\n",
    "\n",
    "traindf2 = visitor_history(traindf2)\n",
    "traindf2 = traindf2.drop(columns=['visitor_hist_starrating', 'visitor_hist_adr_usd', 'gross_bookings_usd', 'srch_query_affinity_score', 'orig_destination_distance'])\n",
    "\n",
    "\n",
    "print(\"Percentage train: \", calculate_missing_percentage(traindf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage train:  0.8835\n"
     ]
    }
   ],
   "source": [
    "testdf2 = competitors(testdf)\n",
    "testdf2 = testdf2.drop(columns=['comp1_rate', 'comp1_inv',\n",
    "       'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv',\n",
    "       'comp2_rate_percent_diff', 'comp3_rate', 'comp3_inv',\n",
    "       'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv',\n",
    "       'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv',\n",
    "       'comp5_rate_percent_diff', 'comp6_rate', 'comp6_inv',\n",
    "       'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv',\n",
    "       'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv',\n",
    "       'comp8_rate_percent_diff'])\n",
    "\n",
    "testdf2 = visitor_history(testdf2)\n",
    "testdf2 = testdf2.drop(columns=['visitor_hist_starrating', 'visitor_hist_adr_usd', 'srch_query_affinity_score', 'orig_destination_distance'])\n",
    "\n",
    "print(\"Percentage train: \", calculate_missing_percentage(testdf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_score(df):\n",
    "    \"\"\"\n",
    "    Add an importance score based on click_bool and booking_bool\n",
    "    \"\"\"\n",
    "    \n",
    "    # every hotel that is clicked on gets an importance score of 1\n",
    "    df[\"importance\"] = df[\"click_bool\"]\n",
    "    \n",
    "    # every hotel that is booked gets an importance score of 5 \n",
    "    df[\"importance\"][df[\"booking_bool\"] == 1] = 5\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pos(traindf, testdf):\n",
    "    \n",
    "    # Only select non-random \n",
    "    dest_dict = traindf.loc[traindf[\"random_bool\"] == 0]\n",
    "    \n",
    "    dest_dict = traindf.groupby([\"srch_destination_id\", \"prop_id\"]).agg(\n",
    "        {\"position\": \"mean\"}\n",
    "    )\n",
    "    dest_dict = dest_dict.rename(\n",
    "        index=str, columns={\"position\": \"estimated_position\"}).reset_index()\n",
    "    \n",
    "    dest_dict[\"srch_destination_id\"] = (\n",
    "        dest_dict[\"srch_destination_id\"].astype(str).astype(int)\n",
    "    )\n",
    "    dest_dict[\"prop_id\"] = (\n",
    "        dest_dict[\"prop_id\"].astype(str).astype(int))\n",
    "    \n",
    "    dest_dict[\"estimated_position\"] = (1 / dest_dict[\"estimated_position\"])\n",
    "    \n",
    "    testdf = testdf.merge(dest_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"])\n",
    "    \n",
    "    traindf = traindf.merge(dest_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"])\n",
    "    \n",
    "    return testdf, traindf\n",
    "\n",
    "def estimate_importance(traindf, testdf):\n",
    "    \n",
    "    # Only select non-random \n",
    "    dest_dict = traindf.loc[traindf[\"random_bool\"] == 0]\n",
    "    \n",
    "    dest_dict = traindf.groupby([\"srch_destination_id\", \"prop_id\"]).agg(\n",
    "        {\"importance\": \"mean\"}\n",
    "    )\n",
    "    dest_dict = dest_dict.rename(\n",
    "        index=str, columns={\"importance\": \"estimated_importance\"}).reset_index()\n",
    "    \n",
    "    dest_dict[\"srch_destination_id\"] = (\n",
    "        dest_dict[\"srch_destination_id\"].astype(str).astype(int)\n",
    "    )\n",
    "    dest_dict[\"prop_id\"] = (\n",
    "        dest_dict[\"prop_id\"].astype(str).astype(int))\n",
    "    \n",
    "    dest_dict[\"estimated_importance\"] = (1 / dest_dict[\"estimated_importance\"])\n",
    "    \n",
    "    testdf = testdf.merge(dest_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"])\n",
    "    \n",
    "    traindf = traindf.merge(dest_dict, how=\"left\", on=[\"srch_destination_id\", \"prop_id\"])\n",
    "    \n",
    "    return testdf, traindf\n",
    "\n",
    "\n",
    "\n",
    "traindf = add_score(traindf)\n",
    "print(\"added score\")\n",
    "    \n",
    "testdf, traindf = estimate_pos(traindf, testdf)\n",
    "testdf, traindf = estimate_importance(traindf, testdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(input_df, group_key, target_column, take_log10=False):\n",
    "\n",
    "    # for numerical stability\n",
    "    epsilon = 1e-4\n",
    "    if take_log10:\n",
    "        input_df[target_column] = np.log10(input_df[target_column] + epsilon)\n",
    "    methods = [\"mean\", \"std\"]\n",
    "\n",
    "    df = input_df.groupby(group_key).agg({target_column: methods})\n",
    "\n",
    "    df.columns = df.columns.droplevel()\n",
    "    col = {}\n",
    "    for method in methods:\n",
    "        col[method] = target_column + \"_\" + method\n",
    "\n",
    "    df.rename(columns=col, inplace=True)\n",
    "    df_merge = input_df.merge(df.reset_index(), on=group_key)\n",
    "    df_merge[target_column + \"_norm_by_\" + group_key] = (\n",
    "        df_merge[target_column] - df_merge[target_column + \"_mean\"]\n",
    "    ) / df_merge[target_column + \"_std\"]\n",
    "    df_merge = df_merge.drop(labels=[col[\"mean\"], col[\"std\"]], axis=1)\n",
    "\n",
    "    return df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing_set is True:\n",
    "    filename = \"./data/test_clean.hdf\"\n",
    "    traindf = testdf\n",
    "else:\n",
    "    filename = \"./data/traindf_clean.hdf\"\n",
    "    \n",
    "    traindf = traindf[traindf.random_bool == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsampling(df):\n",
    "    \"\"\"\n",
    "    Balance classes in trainingset, based on click_bool (not booking_bool)\n",
    "    \"\"\"\n",
    "    \n",
    "    length = len(df.loc[df.importance == 5])\n",
    "    length1 = len(df.loc[df.importance == 1])\n",
    "\n",
    "    \n",
    "    # Get 50% of data with importance of 5 or 1\n",
    "    clicks = df[df.importance == 5].index\n",
    "    randoms = np.random.choice(clicks, length , replace=False)\n",
    "    click_sample = df.loc[randoms]\n",
    "    \n",
    "\n",
    "    not_click = df[df.importance == 1].index\n",
    "    random_indices = np.random.choice(not_click, length1, replace=False)\n",
    "    not_click_sample = df.loc[random_indices]\n",
    "    print(len(random_indices))\n",
    "    \n",
    "    not_click = df[df.importance == 0].index\n",
    "    random_indices = np.random.choice(not_click, length1, replace=False)\n",
    "    not_click_sample2 = df.loc[random_indices]\n",
    "    print(len(random_indices))\n",
    "\n",
    "    df_new = pd.concat([click_sample, not_click_sample, not_click_sample2], axis=0)\n",
    "    \n",
    "    print(\"Percentage of not click impressions: \", len(df_new[df_new.importance == 0])/len(df))\n",
    "    print(\"Percentage of click impression: \", len(df_new[df_new.importance != 0])/len(df))\n",
    "    \n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def competitors(df):\n",
    "    \"\"\"\n",
    "    Make a new column in the dataframe (competitor_bool) for when there \n",
    "    exists a competitor and there are available rooms.\n",
    "    1 is True, 0 is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # we say that there is no competitor with a lower price\n",
    "    df[\"competitor_lower\"] = 0 #competitor_bools\n",
    "    \n",
    "    # comp1rate = 1 if price is lower\n",
    "    df[\"competitor_lower\"][df[\"comp1_rate\"] == 1] = 1\n",
    "    df[\"competitor_lower\"][df[\"comp2_rate\"] == 1] = 1\n",
    "    df[\"competitor_lower\"][df[\"comp3_rate\"] == 1] = 1\n",
    "    df[\"competitor_lower\"][df[\"comp4_rate\"] == 1] = 1\n",
    "    df[\"competitor_lower\"][df[\"comp5_rate\"] == 1] = 1\n",
    "    df[\"competitor_lower\"][df[\"comp6_rate\"] == 1] = 1\n",
    "    df[\"competitor_lower\"][df[\"comp7_rate\"] == 1] = 1\n",
    "    df[\"competitor_lower\"][df[\"comp8_rate\"] == 1] = 1\n",
    "    \n",
    "    # we say at first there is no competitor hotel available\n",
    "    df[\"competitor_available\"] = 0\n",
    "    \n",
    "    # availability bool = 1 if there if the competitor and expedia are available\n",
    "    df[\"competitor_available\"][df[\"comp1_inv\"] == 1] = 1\n",
    "    df[\"competitor_available\"][df[\"comp2_inv\"] == 1] = 1\n",
    "    df[\"competitor_available\"][df[\"comp3_inv\"] == 1] = 1\n",
    "    df[\"competitor_available\"][df[\"comp4_inv\"] == 1] = 1\n",
    "    df[\"competitor_available\"][df[\"comp5_inv\"] == 1] = 1\n",
    "    df[\"competitor_available\"][df[\"comp6_inv\"] == 1] = 1\n",
    "    df[\"competitor_available\"][df[\"comp7_inv\"] == 1] = 1\n",
    "    df[\"competitor_available\"][df[\"comp8_inv\"] == 1] = 1\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visitor_history(df):\n",
    "    \"\"\"\n",
    "    Add column that tells us whether someone has visited a hotel before.\n",
    "    Column name = total_visited; 1 is True, 0 is False.\n",
    "    \"\"\"\n",
    "    \n",
    "    # most visitors haven't visited a hotel yet\n",
    "    df[\"visited_before\"] = 0\n",
    "    \n",
    "    # where there is a history field filled in, visited_before is turned into 21\n",
    "    df[\"visited_before\"][df[\"visitor_hist_starrating\"].notna() | df[\"visitor_hist_adr_usd\"].notna()] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_quality(df):\n",
    "    \"\"\"\n",
    "    Add a column of ratio price/quality to the DataFrame.\n",
    "    Also add review/quality to the df.\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"price_quality\"] = np.nan\n",
    "    df[\"price_review\"] = np.nan\n",
    "    \n",
    "    df[\"price_quality\"][df[\"price_correction\"].notna() & df[\"prop_starrating\"].notna() & df[\"prop_starrating\"] != 0] = df[\"price_correction\"] / df[\"prop_starrating\"]\n",
    "    df[\"price_review\"][df[\"price_correction\"].notna() & df[\"prop_review_score\"].notna() & df[\"prop_review_score\"] != 0] = df[\"price_correction\"] / df[\"prop_review_score\"]\n",
    "\n",
    "    # Replace missing values with median\n",
    "#     df[\"price_quality\"].fillna((df[\"price_quality\"].median()), inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_category(df):\n",
    "    \"\"\"\n",
    "    Add a column of categories of price_usd and a column\n",
    "    that corrected price for number of nights.\n",
    "    Preprocessing of quantile cut showed that categories are:\n",
    "    [(6.0889999999999995, 69.0] < (69.0, 90.0] < (90.0, 110.0] \n",
    "    < (110.0, 136.0] < (136.0, 170.077] < (170.077, 239.0] < (239.0, 554655.0]]\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    columnames = list(df.columns)\n",
    "    columnames.extend([\"avg_price_propid\", \"std_avg_price_propid\", \"amount_hotels\", \n",
    "                       \"avg_price_propid_after\", \"std_avg_price_propid_after\"])\n",
    "    \n",
    "    df = df.reindex(columns=columnames)\n",
    "    \n",
    "    # copy prices (for now, at the end we will just update the price i suppose)\n",
    "    df[\"price_correction\"] = df[\"price_usd\"]\n",
    "    \n",
    "    print(\"Made all extra columns\")\n",
    "    display(df[[\"avg_price_propid\", \"std_avg_price_propid\", \"amount_hotels\", \n",
    "                       \"avg_price_propid_after\", \"std_avg_price_propid_after\", \"price_correction\"]])\n",
    "    \n",
    "    amount = 0\n",
    "    \n",
    "    propids = list(df.prop_id.unique())\n",
    "    \n",
    "    for prop_id in tqdm(propids, desc=\"Processing propids:\"): \n",
    "        # calculate average and standard deviation\n",
    "        std = df[\"price_usd\"][df[\"prop_id\"] == prop_id].std()\n",
    "        avg = df[\"price_usd\"][df[\"prop_id\"] == prop_id].mean() \n",
    "        \n",
    "        # count how many times this hotel appears in the dataframe\n",
    "        df[\"amount_hotels\"][df[\"prop_id\"] == prop_id] = len(df[df[\"prop_id\"] == prop_id])\n",
    "        \n",
    "        # put average and standard deviation in dataframe\n",
    "        df[\"avg_price_propid\"][df[\"prop_id\"] == prop_id] = avg\n",
    "        df[\"std_avg_price_propid\"][df[\"prop_id\"] == prop_id] = std\n",
    "        \n",
    "        # If std is high, correct for number of nights\n",
    "        if std > 50:\n",
    "            amount += 1\n",
    "            df[\"price_correction\"][df[\"prop_id\"] == prop_id] = df[\"price_usd\"][df[\"prop_id\"] == prop_id] / df[\"srch_length_of_stay\"][df[\"prop_id\"] == prop_id]\n",
    "        \n",
    "        # for now separate columns so we can compare\n",
    "        df[\"avg_price_propid_after\"][df[\"prop_id\"] == prop_id] = df[\"price_correction\"][df[\"prop_id\"] == prop_id].mean()\n",
    "        df[\"std_avg_price_propid_after\"][df[\"prop_id\"] == prop_id] = df[\"price_correction\"][df[\"prop_id\"] == prop_id].std()\n",
    "            \n",
    "    print(\"Amount of properties with std > 50: \", amount)\n",
    "    \n",
    "    # Ik kreeg hier een error sorry\n",
    "#     print(\"Correcting \", len(df[df[\"std_avg_price_propid\"] > 50]), \" property prices\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_remaining_cols(df):\n",
    "    \"\"\"\n",
    "    Add some remaining (and interesting columns) to the dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Replace missing values with median\n",
    "    df[\"prop_brand_bool\"].fillna((df[\"prop_brand_bool\"].median()), inplace=True)\n",
    "    df[\"random_bool\"].fillna((df[\"random_bool\"].median()), inplace=True)\n",
    "    \n",
    "    df[\"total_loc_score\"] = df[\"prop_location_score2\"] + df[\"prop_location_score1\"]\n",
    "\n",
    "    df[\"prop_location_score1\"].fillna(-1, inplace=True)\n",
    "    df[\"prop_location_score2\"].fillna(0, inplace=True)\n",
    "    \n",
    "    df[\"prop_starrating\"].fillna(0, inplace=True)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_rank(df):\n",
    "    \"\"\"\n",
    "    Add the rank for every prop_id within each srch_id\n",
    "    \"\"\"\n",
    "    df[\"price_rank\"] = df.groupby(\"srch_id\")[\"price_usd\"].rank()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locationscore_rank(df):\n",
    "    \"\"\"\n",
    "    Add the rank for every location score within each srch_id\n",
    "    \"\"\"\n",
    "    df[\"locationscore2_rank\"] = df.groupby(\"srch_id\")[\"prop_location_score2\"].rank()\n",
    "    df[\"locationscore1_rank\"] = df.groupby(\"srch_id\")[\"prop_location_score1\"].rank()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starrating(df):\n",
    "    \"\"\"\n",
    "    Also add starrating rank and mean starrating for each property\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"starrating_rank\"] = df.groupby(\"srch_id\")[\"prop_starrating\"].rank()\n",
    "    \n",
    "    \n",
    "    df[\"mean_rating_propid\"] = df.groupby(\"prop_id\")[\"prop_starrating\"].transform('mean')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_diff_hist(df):\n",
    "    \n",
    "    df[\"prop_log_historical_prices\"] =  10 ** df[\"prop_log_historical_price\"]\n",
    "    \n",
    "    df[\"price_diff_hist\"] = df[\"price_usd\"] - df[\"prop_log_historical_prices\"]\n",
    "    df[\"diff_price_srchid\"] = df[\"price_correction\"] - df['avg_price_propid_after']\n",
    "    df[\"diff_price_propid\"] = df[\"price_correction\"] - df['avg_price_srchid']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aparte functies aanroepen voor de kolommen die je erbij wilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZATION\n",
    "traindf = normalize_features(traindf, group_key=\"prop_id\", target_column=\"price_usd\", take_log10=False)\n",
    "print(\"Normalized price_usd by prop_id\")\n",
    "\n",
    "traindf = normalize_features(traindf, group_key=\"srch_id\", target_column=\"prop_starrating\")\n",
    "print(\"Normalized prop_starrating by srch_id\")\n",
    "\n",
    "traindf = normalize_features(traindf, group_key=\"srch_id\", target_column=\"prop_location_score2\")\n",
    "print(\"Normalized prop_locationscore2 by srch_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindf = competitors(traindf)\n",
    "# print(\"cleaned competitors\")\n",
    "\n",
    "# traindf = visitor_history(traindf)\n",
    "# print(\"cleaned visitor history\")\n",
    "\n",
    "traindf = process_remaining_cols(traindf)\n",
    "print(\"did remaining columns\")\n",
    "\n",
    "traindf = price_quality(traindf)\n",
    "print(\"cleaned price quality\")\n",
    "\n",
    "traindf = price_rank(traindf)\n",
    "print(\"added a rank of price per search_id\")\n",
    "\n",
    "traindf = locationscore_rank(traindf)\n",
    "print(\"added a rank of location score search_id\")\n",
    "\n",
    "traindf = starrating(traindf)\n",
    "print(\"added starrating and mean of prop_ids\")\n",
    "\n",
    "traindf = price_diff_hist(traindf)\n",
    "print(\"added history of price info\")\n",
    "\n",
    "# Add relevant columns    \n",
    "if testing_set is False:\n",
    "    \n",
    "    df = traindf[[\"prop_id\", \n",
    "                  \"srch_id\", \n",
    "                  \"position\", \n",
    "                  \"price_quality\", \n",
    "                  \"price_review\",\n",
    "                  \"click_bool\", \n",
    "                  \"booking_bool\",  \n",
    "                  \"price_usd\",\n",
    "#                   \"prop_location_score1\", \n",
    "                  \"prop_location_score2\",\n",
    "                  \"avg_price_propid\", \n",
    "                  \"std_avg_price_propid\", \n",
    "                  \"amount_hotels\", \n",
    "                  \"avg_price_propid_after\", \n",
    "                  \"price_rank\",\n",
    "#                   \"price_correction\",\n",
    "                  \"locationscore2_rank\",\n",
    "                  \"locationscore1_rank\",\n",
    "                  \"starrating_rank\",\n",
    "                  \"price_diff_hist\",\n",
    "                  \"diff_price_srchid\",\n",
    "                  \"diff_price_propid\",\n",
    "                  \"total_loc_score\",\n",
    "                  \"estimated_position\",\n",
    "                   \"price_usd_norm_by_prop_id\",\n",
    "                  \"prop_starrating_norm_by_srch_id\",\n",
    "                  \"prop_location_score2_norm_by_srch_id\",\n",
    "                  \"estimated_importance\"\n",
    "                 ]]\n",
    "    \n",
    "    df = add_score(df)\n",
    "    print(\"added score\")\n",
    "    \n",
    "    # Balance data to 50% importance score or 1 or 5 and 0 \n",
    "#     df = downsampling(df)\n",
    "#     print(\"Downsampled data\")\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # df without click_bool, booking_bool and position\n",
    "    df = traindf[[\"prop_id\", \n",
    "                  \"srch_id\", \n",
    "                  \"price_quality\", \n",
    "                  \"price_review\",\n",
    "                  \"price_usd\",\n",
    "#                   \"prop_location_score1\", \n",
    "                  \"prop_location_score2\",\n",
    "                  \"avg_price_propid\", \n",
    "                  \"std_avg_price_propid\", \n",
    "                  \"amount_hotels\", \n",
    "                  \"avg_price_propid_after\", \n",
    "                  \"price_rank\",\n",
    "#                   \"price_correction\",\n",
    "                  \"locationscore2_rank\",\n",
    "                  \"locationscore1_rank\",\n",
    "                  \"starrating_rank\",\n",
    "                  \"price_diff_hist\",\n",
    "                  \"diff_price_srchid\",\n",
    "                  \"diff_price_propid\",\n",
    "                  \"total_loc_score\",\n",
    "                  \"estimated_position\",\n",
    "                  \"price_usd_norm_by_prop_id\",\n",
    "                  \"prop_starrating_norm_by_srch_id\",\n",
    "                  \"prop_location_score2_norm_by_srch_id\",\n",
    "                  \"estimated_importance\"\n",
    "                 ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"price_quality\"] = df.price_quality.astype(np.float32)\n",
    "print(df.dtypes)\n",
    "print()\n",
    "\n",
    "if df.isnull().sum().sum() != 0:    \n",
    "    print(\"\\x1b[31mMissing values: \\'\\x1b[0m\")\n",
    "    print(df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\x1b[31mNo missing values!! :D \\'\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Totale dataset gepreprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe if it does not exist yet\n",
    "# if not os.path.exists(filename):\n",
    "#     df.to_hdf(filename, key=\"df\", format=\"table\")\n",
    "df.to_hdf(filename, key=\"df\", format=\"table\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
