{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Research and theory\n",
    "### Task 3A: Research - State of the art solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3B: Theory - MSE versus MAE\n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "$y_i$ is the actual expected output and $\\hat{y}_i$ is the model's prediction.\n",
    "\n",
    "MSE measures averages squared error of our predictions. For each point, it calculates square difference between the predictions and the target and then average those values.\n",
    "\n",
    "The higher this value, the worse the model is. It's never negative since we're squaring the individual prediction-wise error before summing them, but would be zero for a perfect model. \n",
    "\n",
    "*Advantage*: Useful if we have unexpected values that we should care about. Vey high or low value that we should pay attention.\n",
    "\n",
    "*Disadvantage*: If we make a single very bad prediction, the squaring will make the error even worse and it may skew the metric towards overestimating the model’s badness. That is a particularly problematic behaviour if we have noisy data (that is, data that for whatever reason is not entirely reliable) — even a “perfect” model may have a high MSE in that situation, so it becomes hard to judge how well the model is performing. On the other hand, if all the errors are small, or rather, smaller than 1, than the opposite effect is felt: we may underestimate the model’s badness.\n",
    "\n",
    "*Note that* if we want to have a constant prediction the best one will be the mean value of the target values. It can be found by setting the derivative of our total error with respect to that constant to zero, and find it from this equation.\n",
    "\n",
    "\n",
    "$$ MAE = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y}_i| $$\n",
    "\n",
    "MAE calculates the error as an average of absolute differences between the target values and the predictions. The MAE is a linear score which means that all the individual differences are weighted equally in the average. For example, the difference between 10 and 0 will be twice the difference between 5 and 0.\n",
    "\n",
    "What is important about this metric is that it penalizes huge errors that not as that badly as MSE does. Thus, it’s not that sensitive to outliers as mean square error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE – Mean Absolute Error\n",
    "MAE is the most intuitive of them all. The name in itself is pretty good at telling us what’s going on.\n",
    "\n",
    "- Mean: average\n",
    "- Absolute: without direction, get rid of any negative signs\n",
    "Simply put, the average difference observed in the predicted and actual values across the whole test set.\n",
    "\n",
    "In the background, the algorithm takes the differences in all of the predicted and actual prices, adds them up and then divides them by the number of observations. It doesn’t matter if the prediction is higher or lower than the actual price, the algorithm just looks at the absolute value. A lower value indicates better accuracy.\n",
    "\n",
    "As a general guide, I think we can use MAE when we aren’t too worried about the outliers.\n",
    "\n",
    "#### Mean Squared Error\n",
    "I personally don’t focus too much on MSE as I see it as a stepping stone for calculating RMSE. However, let’s see what’s it about.\n",
    "\n",
    "- Mean: average\n",
    "- Squared: square the errors so a difference of 2, becomes 4, a difference of 3 becomes 9\n",
    "As you can see, as a result of the squaring, it assigns more weight to the bigger errors. The algorithm then continues to add them up and average them. If you are worried about the outliers, this is the number to look at. Keep in mind, it’s not in the same unit as our dependent value. In our case, the value was roughly 82,3755,495, this is NOT the dollar value of the error like MAE. As before, lower the number the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3C: Theory - analyze a less obvious dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\" Reads a file. \"\"\"\n",
    "    \n",
    "    f = open(filename, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "def create_dataframe(lines):\n",
    "    \"\"\" Create a dataframe from a csv file. \"\"\"\n",
    "    \n",
    "    # get column names from first line\n",
    "    col_names = lines[0].split(';')\n",
    "    cols = [col_names[i].strip() for i in range(len(col_names))]\n",
    "    \n",
    "    # prepare data frame\n",
    "    amount_lines = len(lines)\n",
    "    df = pd.DataFrame(columns=cols, index=range(amount_lines - 1))\n",
    "    \n",
    "    # fill dataframe\n",
    "    i = 0\n",
    "    for line in lines[1:]:\n",
    "\n",
    "        parts = line.split(';', 1)\n",
    "\n",
    "        df.loc[i].label = parts[0]\n",
    "        df.loc[i].text = parts[1].strip()\n",
    "\n",
    "        i = i + 1\n",
    "        \n",
    "    return df\n",
    "\n",
    "def create_dataframe_stratified(lines):\n",
    "    \"\"\" Create a dataframe from a csv file. \"\"\"\n",
    "    \n",
    "    # get column names from first line\n",
    "    col_names = lines[0].split(';')\n",
    "    cols = [col_names[i].strip() for i in range(len(col_names))]\n",
    "    \n",
    "    # prepare data frame\n",
    "    amount_lines = len(lines)\n",
    "    df = pd.DataFrame(columns=cols, index=range(9309))\n",
    "    \n",
    "    # fill dataframe\n",
    "    i = 0\n",
    "    for line in lines[1:]:\n",
    "\n",
    "        parts = line.split(';', 1)\n",
    "        \n",
    "        if parts[0] == \"spam\":\n",
    "            for j in range(6):\n",
    "                df.loc[i + j].label = parts[0]\n",
    "                df.loc[i + j].text = parts[1].strip()\n",
    "            \n",
    "            i = i + 6\n",
    "                \n",
    "        else:\n",
    "            df.loc[i].label = parts[0]\n",
    "            df.loc[i].text = parts[1].strip()\n",
    "\n",
    "            i = i + 1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5574</td>\n",
       "      <td>5574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4827</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                    text\n",
       "count   5574                    5574\n",
       "unique     2                    5160\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4827                      30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9309</td>\n",
       "      <td>9309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4827</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                    text\n",
       "count   9309                    9309\n",
       "unique     2                    5160\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4827                      30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lines = read_file(\"data/SmsCollection.csv\")\n",
    "df = create_dataframe(lines)\n",
    "df_stratified = create_dataframe_stratified(lines)\n",
    "\n",
    "display(df.describe())\n",
    "display(df_stratified.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mac, uncomment:\n",
    "# import nltk\n",
    "# import ssl\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_messages(msg, unescape_html, lower, rm_stopwords, use_stemmer):\n",
    "    \"\"\" This is a function that converts messages to all lowercase letters, removes stopwords\n",
    "        and stems all words: stemming reduces inflection forms to normalise words with the same lemma.\"\"\"\n",
    "    \n",
    "    if unescape_html:\n",
    "        # unescape html\n",
    "        msg = html.unescape(msg)\n",
    "    \n",
    "    if lower:\n",
    "        # converting messages to lowercase\n",
    "        msg = msg.lower()\n",
    "    \n",
    "    if rm_stopwords:\n",
    "        # removing stopwords\n",
    "        msg = [word for word in msg.split() if word not in stopwords]\n",
    "        \n",
    "    if rm_stopwords and use_stemmer:\n",
    "        # using a stemmer\n",
    "        msg = \" \".join([stemmer.stem(word) for word in msg])\n",
    "    elif rm_stopwords:\n",
    "        msg = \" \".join([word for word in msg])\n",
    "    \n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show\n",
    "def display_big(df):\n",
    "    with pd.option_context('display.min_rows', 50, 'display.max_colwidth', 10000):\n",
    "        display(df)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# use term frequency - inverse document frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "def clean_and_split_data(df, test_size, clean_options, stratified):\n",
    "    df_train = df.copy()\n",
    "    \n",
    "    \n",
    "    df_train[\"text\"] = df_train[\"text\"].apply(clean_messages, args=(clean_options))\n",
    "    \n",
    "    # Use stratified sampling to split test- and training set\n",
    "    if stratified is True:\n",
    "        \n",
    "        split_model = StratifiedShuffleSplit(n_splits = 1, test_size = test_size, random_state = 1)\n",
    "        \n",
    "        for train_index, test_index in split_model.split(df_train['text'], df_train['label']):\n",
    "                trainmsg, testmsg = df_train['text'][train_index], df_train['text'][test_index]\n",
    "                trainlabel, testlabel = df_train['label'][train_index], df_train['label'][test_index]\n",
    "\n",
    "\n",
    "    else:\n",
    "        # split dataset into test and training\n",
    "        trainmsg, testmsg, trainlabel, testlabel = train_test_split(\n",
    "                    df_train['text'], \n",
    "                    df_train['label'], \n",
    "                    test_size = test_size, \n",
    "                    random_state = 1)\n",
    "    \n",
    "   \n",
    "\n",
    "    return trainmsg, testmsg, trainlabel, testlabel\n",
    "\n",
    "\n",
    "\n",
    "def train_model(trainmsg, trainlabel, C):\n",
    "        \n",
    "    # vectorize\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    trainmsg = vectorizer.fit_transform(trainmsg)\n",
    "    \n",
    "    # actually classify and train\n",
    "    my_svm = svm.SVC(C=C) #, gamma='auto')\n",
    "    my_svm.fit(trainmsg, trainlabel)\n",
    "    \n",
    "    return vectorizer, my_svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "stratified = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(df, clean_options, test_size, C, stratified):\n",
    "    trainmsg, testmsg, trainlabel, testlabel = clean_and_split_data(df, test_size=test_size, \n",
    "                                                                    clean_options=clean_options,\n",
    "                                                                    stratified=stratified)\n",
    "\n",
    "    vectorizer, my_svm = train_model(trainmsg=trainmsg, \n",
    "                                        trainlabel=trainlabel,\n",
    "                                        C=C)\n",
    "\n",
    "    testmsg = vectorizer.transform(testmsg)\n",
    "\n",
    "    y_pred = my_svm.predict(testmsg)\n",
    "    m = confusion_matrix(testlabel, y_pred)\n",
    "    print(m)\n",
    "    \n",
    "    n = (m[1][0] + m[1][1] + m[0][0] + m[0][1])\n",
    "    positives_error = m[1][0] / (m[1][0] + m[1][1])\n",
    "    negatives_error = m[0][1] / (m[0][0] + m[0][1])\n",
    "    \n",
    "    print(\"False positives: \",  round(positives_error * 100, 1), \"% +/-\", get_ci(n, positives_error), \"%\")\n",
    "    print(\"False negatives: \",  round(negatives_error * 100, 1), \"% +/-\", get_ci(n, negatives_error), \"%\")\n",
    "    \n",
    "def get_ci(n, error, confidence=0.99):\n",
    "    \n",
    "    if confidence==0.99:\n",
    "        const = 2.58\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return round(const * ((error * (1-error))/n)**0.5 * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_options1 = [False, # html\n",
    "                    False, # lower\n",
    "                    False, # stopwords\n",
    "                    False] # stemmer\n",
    "\n",
    "clean_options2 = [True, # html\n",
    "                    True, # lower\n",
    "                    True, # stopwords\n",
    "                    True] # stemmer\n",
    "\n",
    "clean_options3 = [True, # html\n",
    "                    True, # lower\n",
    "                    False, # stopwords\n",
    "                    False] # stemmer\n",
    "\n",
    "clean_options4 = [False, # html\n",
    "                    False, # lower\n",
    "                    True, # stopwords\n",
    "                    True] # stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[977   0]\n",
      " [ 13 125]]\n",
      "False positives:  9.4 % +/- 2.3 %\n",
      "False negatives:  0.0 % +/- 0.0 %\n",
      "[[976   1]\n",
      " [ 12 126]]\n",
      "False positives:  8.7 % +/- 2.2 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[490   1]\n",
      " [  5  62]]\n",
      "False positives:  7.5 % +/- 2.9 %\n",
      "False negatives:  0.2 % +/- 0.5 %\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "C = 1\n",
    "\n",
    "run_model(df, clean_options1, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "C = 1000\n",
    "\n",
    "run_model(df, clean_options1, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "C = 1000\n",
    "\n",
    "run_model(df, clean_options1, test_size, C, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[976   1]\n",
      " [ 14 124]]\n",
      "False positives:  10.1 % +/- 2.3 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[976   1]\n",
      " [ 11 127]]\n",
      "False positives:  8.0 % +/- 2.1 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[491   0]\n",
      " [  5  62]]\n",
      "False positives:  7.5 % +/- 2.9 %\n",
      "False negatives:  0.0 % +/- 0.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_size = 0.2\n",
    "C = 1\n",
    "\n",
    "run_model(df, clean_options2, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "C = 1000\n",
    "\n",
    "run_model(df, clean_options2, test_size, C, stratified)\n",
    "\n",
    "test_size = 0.1\n",
    "C = 1000\n",
    "\n",
    "run_model(df, clean_options2, test_size, C, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[977   0]\n",
      " [ 13 125]]\n",
      "False positives:  9.4 % +/- 2.3 %\n",
      "False negatives:  0.0 % +/- 0.0 %\n",
      "[[976   1]\n",
      " [ 12 126]]\n",
      "False positives:  8.7 % +/- 2.2 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[490   1]\n",
      " [  6  61]]\n",
      "False positives:  9.0 % +/- 3.1 %\n",
      "False negatives:  0.2 % +/- 0.5 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_size = 0.2\n",
    "C = 1\n",
    "\n",
    "run_model(df, clean_options3, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "C = 1000\n",
    "\n",
    "run_model(df, clean_options3, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "C = 1000\n",
    "\n",
    "run_model(df, clean_options3, test_size, C, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[976   1]\n",
      " [ 14 124]]\n",
      "False positives:  10.1 % +/- 2.3 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[976   1]\n",
      " [ 11 127]]\n",
      "False positives:  8.0 % +/- 2.1 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[491   0]\n",
      " [  6  61]]\n",
      "False positives:  9.0 % +/- 3.1 %\n",
      "False negatives:  0.0 % +/- 0.0 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_size = 0.2\n",
    "C = 1\n",
    "\n",
    "run_model(df, clean_options4, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "C = 1000\n",
    "\n",
    "run_model(df, clean_options4, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "C = 1000\n",
    "\n",
    "run_model(df, clean_options4, test_size, C, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[943   2]\n",
      " [  0 917]]\n",
      "False positives:  0.0 % +/- 0.0 %\n",
      "False negatives:  0.2 % +/- 0.3 %\n",
      "[[944   1]\n",
      " [  0 917]]\n",
      "False positives:  0.0 % +/- 0.0 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[482   0]\n",
      " [  0 449]]\n",
      "False positives:  0.0 % +/- 0.0 %\n",
      "False negatives:  0.0 % +/- 0.0 %\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "C = 1\n",
    "\n",
    "run_model(df_stratified, clean_options1, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "C = 1000\n",
    "\n",
    "run_model(df_stratified, clean_options1, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "C = 1000\n",
    "\n",
    "run_model(df_stratified, clean_options1, test_size, C, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[944   1]\n",
      " [  0 917]]\n",
      "False positives:  0.0 % +/- 0.0 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[944   1]\n",
      " [  0 917]]\n",
      "False positives:  0.0 % +/- 0.0 %\n",
      "False negatives:  0.1 % +/- 0.2 %\n",
      "[[482   0]\n",
      " [  0 449]]\n",
      "False positives:  0.0 % +/- 0.0 %\n",
      "False negatives:  0.0 % +/- 0.0 %\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "C = 1\n",
    "\n",
    "run_model(df_stratified, clean_options2, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "C = 1000\n",
    "\n",
    "run_model(df_stratified, clean_options2, test_size, C, stratified)\n",
    "\n",
    "\n",
    "test_size = 0.1\n",
    "C = 1000\n",
    "\n",
    "run_model(df_stratified, clean_options2, test_size, C, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
