{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from random import randrange\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Research and theory\n",
    "### Task 3A: Research - State of the art solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3B: Theory - MSE versus MAE\n",
    "\n",
    "$$ MSE = \\frac{1}{N}\\sum_{i=1}^{N}(y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "$y_i$ is the actual expected output and $\\hat{y}_i$ is the model's prediction.\n",
    "\n",
    "MSE measures averages squared error of our predictions. For each point, it calculates square difference between the predictions and the target and then average those values.\n",
    "\n",
    "The higher this value, the worse the model is. It's never negative since we're squaring the individual prediction-wise error before summing them, but would be zero for a perfect model. \n",
    "\n",
    "*Advantage*: Useful if we have unexpected values that we should care about. Vey high or low value that we should pay attention.\n",
    "\n",
    "*Disadvantage*: If we make a single very bad prediction, the squaring will make the error even worse and it may skew the metric towards overestimating the model’s badness. That is a particularly problematic behaviour if we have noisy data (that is, data that for whatever reason is not entirely reliable) — even a “perfect” model may have a high MSE in that situation, so it becomes hard to judge how well the model is performing. On the other hand, if all the errors are small, or rather, smaller than 1, than the opposite effect is felt: we may underestimate the model’s badness.\n",
    "\n",
    "*Note that* if we want to have a constant prediction the best one will be the mean value of the target values. It can be found by setting the derivative of our total error with respect to that constant to zero, and find it from this equation.\n",
    "\n",
    "\n",
    "$$ MAE = \\frac{1}{N}\\sum_{i=1}^{N}|y_i - \\hat{y}_i| $$\n",
    "\n",
    "MAE calculates the error as an average of absolute differences between the target values and the predictions. The MAE is a linear score which means that all the individual differences are weighted equally in the average. For example, the difference between 10 and 0 will be twice the difference between 5 and 0.\n",
    "\n",
    "What is important about this metric is that it penalizes huge errors that not as that badly as MSE does. Thus, it’s not that sensitive to outliers as mean square error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE – Mean Absolute Error\n",
    "MAE is the most intuitive of them all. The name in itself is pretty good at telling us what’s going on.\n",
    "\n",
    "- Mean: average\n",
    "- Absolute: without direction, get rid of any negative signs\n",
    "Simply put, the average difference observed in the predicted and actual values across the whole test set.\n",
    "\n",
    "In the background, the algorithm takes the differences in all of the predicted and actual prices, adds them up and then divides them by the number of observations. It doesn’t matter if the prediction is higher or lower than the actual price, the algorithm just looks at the absolute value. A lower value indicates better accuracy.\n",
    "\n",
    "In our case, the MAE was telling us that on average our predictions are off by roughly \\\\$24,213. Is this good or bad? To compare, we can go back to our stats table printed earlier by Python and find the mean house price, it’s roughly \\\\$493,091. Now a simple calculation will tell us that the error is about 5% of mean house price, I think that’s pretty good. However, keep in mind that our training and test sets are pretty tiny and things might change significantly when a larger dataset is used.\n",
    "\n",
    "As a general guide, I think we can use MAE when we aren’t too worried about the outliers.\n",
    "\n",
    "#### Mean Squared Error\n",
    "I personally don’t focus too much on MSE as I see it as a stepping stone for calculating RMSE. However, let’s see what’s it about.\n",
    "\n",
    "- Mean: average\n",
    "- Squared: square the errors so a difference of 2, becomes 4, a difference of 3 becomes 9\n",
    "As you can see, as a result of the squaring, it assigns more weight to the bigger errors. The algorithm then continues to add them up and average them. If you are worried about the outliers, this is the number to look at. Keep in mind, it’s not in the same unit as our dependent value. In our case, the value was roughly 82,3755,495, this is NOT the dollar value of the error like MAE. As before, lower the number the better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3C: Theory - analyze a less obvious dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\" Reads a file. \"\"\"\n",
    "    \n",
    "    f = open(filename, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    return lines\n",
    "\n",
    "\n",
    "def create_dataframe(lines):\n",
    "    \"\"\" Create a dataframe from a csv file. \"\"\"\n",
    "    \n",
    "    # get column names from first line\n",
    "    col_names = lines[0].split(';')\n",
    "    cols = [col_names[i].strip() for i in range(len(col_names))]\n",
    "    \n",
    "    # prepare data frame\n",
    "    amount_lines = len(lines)\n",
    "    df = pd.DataFrame(columns=cols, index=range(amount_lines - 1))\n",
    "    \n",
    "    # fill dataframe\n",
    "    i = 0\n",
    "    for line in lines[1:]:\n",
    "\n",
    "        parts = line.split(';', 1)\n",
    "\n",
    "        df.loc[i].label = parts[0]\n",
    "        df.loc[i].text = parts[1].strip()\n",
    "\n",
    "        i = i + 1\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lines = read_file(\"data/SmsCollection.csv\")\n",
    "\n",
    "import html\n",
    "\n",
    "df = create_dataframe(lines)\n",
    "\n",
    "\n",
    "# unescape html \n",
    "df = df.apply(lambda s: html.unescape(s))\n",
    "\n",
    "with pd.option_context('display.min_rows', 50, 'display.max_colwidth', 10000):\n",
    "    display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
